{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "References:\n",
    "\n",
    "* [Introduction to NLP in Python](https://campus.datacamp.com/courses/introduction-to-natural-language-processing-in-python/regular-expressions-word-tokenization)\n",
    "* [Machine Learning with Python Cookbook](https://www.amazon.com/Machine-Learning-Python-Cookbook-Preprocessing/dp/1491989386)\n",
    "* [Feature Engineering for NLP in Python](https://campus.datacamp.com/courses/feature-engineering-for-nlp-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from collections import defaultdict \n",
    "import itertools\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textatistic import Textatistic\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yo This is inspired by Chris Albons 63 Removing Punctuation example',\n",
       " 'Its in his book linked above']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "text = [\"Yo! This is inspired by Chris Albon's 6.3 Removing Punctuation example.\", \"It's in his book linked above!\"]\n",
    "\n",
    "# sys.maxunicode is \"An integer giving the largest supported code point for a Unicode character.\"\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# There are other ways to remove punction characters, but this method is very fast\n",
    "[string.translate(punctuation) for string in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'The',\n",
       " 'Tragedie',\n",
       " 'of',\n",
       " 'Macbeth',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " '1603',\n",
       " ']']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')[0:10000]\n",
    "\n",
    "word_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'The',\n",
       " 'Tragedie',\n",
       " 'of',\n",
       " 'Macbeth',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " '1603',\n",
       " ']']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[The Tragedie of Macbeth by William Shakespeare 1603]\\n\\n\\nActus Primus.',\n",
       " 'Scoena Prima.',\n",
       " 'Thunder and Lightning.',\n",
       " 'Enter three Witches.',\n",
       " '1.',\n",
       " 'When shall we three meet againe?',\n",
       " 'In Thunder, Lightning, or in Raine?',\n",
       " '2.',\n",
       " \"When the Hurley-burley's done,\\nWhen the Battaile's lost, and wonne\\n\\n   3.\",\n",
       " 'That will be ere the set of Sunne\\n\\n   1.']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1603',\n",
       " 'Macbeth',\n",
       " 'Shakespeare',\n",
       " 'The',\n",
       " 'Tragedie',\n",
       " 'William',\n",
       " '[',\n",
       " ']',\n",
       " 'by',\n",
       " 'of'}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(word_tokenize(text)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMO', 'hashtags', 'are', '#lame', '@mhmazur']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = 'IMO hashtags are #lame @mhmazur'\n",
    "TweetTokenizer().tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASv0lEQVR4nO3df6zd9X3f8edrOCSBVpgftx61zUwXNxGNxo/dZc7YqgU3FT+imD9SRNYVj1nyNLE0aaK1TifthzRNzlaVBnVisuI0ZmUQRsmwEpbFMnTVpEJrCOGXk3FLAdsz+IYAacPahvW9P87HyeFy7Xuu749z88nzIR2dz/fz/Xzv933sq9f53s/5fs83VYUkqS9/ZdwFSJIWn+EuSR0y3CWpQ4a7JHXIcJekDhnuktShkcI9yS8leTLJE0nuSPK2JBcmeSjJVJLPJzm9jX1rW55q6zcs5QuQJL3ZnOGeZC3wi8BkVb0bOA24HvgUcHNVvQN4GdjWNtkGvNz6b27jJEnLaNU8xr09yXeBM4CjwBXAP2jr9wD/GrgV2NLaAHcDv5kkdZKrpc4777zasGHDfGuXpB9qDz/88DeramK2dXOGe1UdSfJrwPPA/wW+AjwMvFJVr7dhh4G1rb0WONS2fT3Jq8C5wDeHf26S7cB2gAsuuIADBw7M93VJ0g+1JM+daN0o0zJnMzgavxD4ceBM4MqFFlVVu6pqsqomJyZmfeORJJ2iUT5Q/Rngj6tquqq+C9wDXA6sTnL8yH8dcKS1jwDrAdr6s4CXFrVqSdJJjRLuzwObkpyRJMBm4CngAeBDbcxW4N7W3tuWaevvP9l8uyRp8c0Z7lX1EIMPRh8BHm/b7AJ+Bfh4kikGc+q72ya7gXNb/8eBHUtQtyTpJLISDqonJyfLD1QlaX6SPFxVk7Ot8wpVSeqQ4S5JHTLcJalDhrskdWjUrx9YsTbs+NLY9v3szmvGtm9JOhmP3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjo0Z7gneWeSR4ce307ysSTnJNmX5On2fHYbnyS3JJlK8liSy5b+ZUiSho1yg+xvVNUlVXUJ8DeB14AvMLjx9f6q2gjs5/s3wr4K2Nge24Fbl6JwSdKJzXdaZjPwR1X1HLAF2NP69wDXtvYW4LYaeBBYneT8RalWkjSS+d6s43rgjtZeU1VHW/sFYE1rrwUODW1zuPUdHeojyXYGR/ZccMEF8yxjZRjXjUK8SYikuYx85J7kdOCDwH+dua6qCqj57LiqdlXVZFVNTkxMzGdTSdIc5jMtcxXwSFW92JZfPD7d0p6Ptf4jwPqh7da1PknSMplPuH+Y70/JAOwFtrb2VuDeof4b2lkzm4BXh6ZvJEnLYKQ59yRnAu8H/slQ907griTbgOeA61r/fcDVwBSDM2tuXLRqJUkjGSncq+o7wLkz+l5icPbMzLEF3LQo1UmSTolXqEpShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6tBI4Z5kdZK7k3w9ycEk701yTpJ9SZ5uz2e3sUlyS5KpJI8luWxpX4IkaaZRj9w/DXy5qt4FXAwcBHYA+6tqI7C/LQNcBWxsj+3ArYtasSRpTnOGe5KzgJ8GdgNU1V9U1SvAFmBPG7YHuLa1twC31cCDwOok5y965ZKkExrlyP1CYBr4rSRfTfKZJGcCa6rqaBvzArCmtdcCh4a2P9z63iDJ9iQHkhyYnp4+9VcgSXqTUcJ9FXAZcGtVXQp8h+9PwQBQVQXUfHZcVbuqarKqJicmJuazqSRpDqOE+2HgcFU91JbvZhD2Lx6fbmnPx9r6I8D6oe3XtT5J0jKZM9yr6gXgUJJ3tq7NwFPAXmBr69sK3Nvae4Eb2lkzm4BXh6ZvJEnLYNWI4z4C3J7kdOAZ4EYGbwx3JdkGPAdc18beB1wNTAGvtbGSpGU0UrhX1aPA5CyrNs8ytoCbFliXJGkBvEJVkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHRgr3JM8meTzJo0kOtL5zkuxL8nR7Prv1J8ktSaaSPJbksqV8AZKkN5vPkfv7quqSqjp+L9UdwP6q2gjsb8sAVwEb22M7cOtiFStJGs1CpmW2AHtaew9w7VD/bTXwILA6yfkL2I8kaZ5GDfcCvpLk4STbW9+aqjra2i8Aa1p7LXBoaNvDre8NkmxPciDJgenp6VMoXZJ0IqtGHPd3q+pIkh8D9iX5+vDKqqokNZ8dV9UuYBfA5OTkvLaVJJ3cSEfuVXWkPR8DvgC8B3jx+HRLez7Whh8B1g9tvq71SZKWyZzhnuTMJD96vA38LPAEsBfY2oZtBe5t7b3ADe2smU3Aq0PTN5KkZTDKtMwa4AtJjo//L1X15SR/CNyVZBvwHHBdG38fcDUwBbwG3LjoVUuSTmrOcK+qZ4CLZ+l/Cdg8S38BNy1KdZKkU+IVqpLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOjRyuCc5LclXk3yxLV+Y5KEkU0k+n+T01v/WtjzV1m9YmtIlSScynyP3jwIHh5Y/BdxcVe8AXga2tf5twMut/+Y2TpK0jEYK9yTrgGuAz7TlAFcAd7che4BrW3tLW6at39zGS5KWyahH7r8B/DLwl235XOCVqnq9LR8G1rb2WuAQQFv/ahsvSVomc4Z7kg8Ax6rq4cXccZLtSQ4kOTA9Pb2YP1qSfuiNcuR+OfDBJM8CdzKYjvk0sDrJqjZmHXCktY8A6wHa+rOAl2b+0KraVVWTVTU5MTGxoBchSXqjOcO9qj5ZVeuqagNwPXB/Vf088ADwoTZsK3Bva+9ty7T191dVLWrVkqSTWsh57r8CfDzJFIM59d2tfzdwbuv/OLBjYSVKkuZr1dxDvq+qfhf43dZ+BnjPLGP+DPi5RahNknSKvEJVkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktShed2sQ9qw40tj2e+zO68Zy36lH1QeuUtShwx3SerQnOGe5G1J/iDJ15I8meTftP4LkzyUZCrJ55Oc3vrf2pan2voNS/sSJEkzjXLk/ufAFVV1MXAJcGWSTcCngJur6h3Ay8C2Nn4b8HLrv7mNkyQtoznDvQb+tC2+pT0KuAK4u/XvAa5t7S1tmbZ+c5IsWsWSpDmNNOee5LQkjwLHgH3AHwGvVNXrbchhYG1rrwUOAbT1rwLnzvIztyc5kOTA9PT0wl6FJOkNRgr3qvp/VXUJsA54D/Cuhe64qnZV1WRVTU5MTCz0x0mShszrbJmqegV4AHgvsDrJ8fPk1wFHWvsIsB6grT8LeGlRqpUkjWSUs2Umkqxu7bcD7wcOMgj5D7VhW4F7W3tvW6atv7+qajGLliSd3ChXqJ4P7ElyGoM3g7uq6otJngLuTPJvga8Cu9v43cB/TjIFfAu4fgnqliSdxJzhXlWPAZfO0v8Mg/n3mf1/BvzcolQnSTolXqEqSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOuSdmH4AjetuSJJ+cHjkLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdWiUG2SvT/JAkqeSPJnko63/nCT7kjzdns9u/UlyS5KpJI8luWypX4Qk6Y1GOXJ/HfhEVV0EbAJuSnIRsAPYX1Ubgf1tGeAqYGN7bAduXfSqJUknNWe4V9XRqnqktf8EOAisBbYAe9qwPcC1rb0FuK0GHgRWJzl/0SuXJJ3QvObck2wALgUeAtZU1dG26gVgTWuvBQ4NbXa49c38WduTHEhyYHp6ep5lS5JOZuRwT/IjwO8AH6uqbw+vq6oCaj47rqpdVTVZVZMTExPz2VSSNIeRwj3JWxgE++1VdU/rfvH4dEt7Ptb6jwDrhzZf1/okSctklLNlAuwGDlbVrw+t2gtsbe2twL1D/Te0s2Y2Aa8OTd9IkpbBKLfZuxz4BeDxJI+2vl8FdgJ3JdkGPAdc19bdB1wNTAGvATcuasWSpDnNGe5V9b+AnGD15lnGF3DTAuuSJC2AV6hKUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SerQKDfI/mySY0meGOo7J8m+JE+357Nbf5LckmQqyWNJLlvK4iVJsxvlyP1zwJUz+nYA+6tqI7C/LQNcBWxsj+3ArYtTpiRpPuYM96r6PeBbM7q3AHtaew9w7VD/bTXwILA6yfmLVawkaTSnOue+pqqOtvYLwJrWXgscGhp3uPW9SZLtSQ4kOTA9PX2KZUiSZrPgD1SrqoA6he12VdVkVU1OTEwstAxJ0pBTDfcXj0+3tOdjrf8IsH5o3LrWJ0laRqca7nuBra29Fbh3qP+GdtbMJuDVoekbSdIyWTXXgCR3AH8fOC/JYeBfATuBu5JsA54DrmvD7wOuBqaA14Abl6BmSdIc5gz3qvrwCVZtnmVsATcttChJ0sJ4haokdchwl6QOGe6S1CHDXZI6ZLhLUofmPFtGWgk27PjSWPb77M5rxrJfaaEMd+kkxvWmAr6xaGGclpGkDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA55haq0QvmVC1oIj9wlqUNLEu5JrkzyjSRTSXYsxT4kSSe26OGe5DTgPwJXARcBH05y0WLvR5J0Yksx5/4eYKqqngFIciewBXhqCfYlaZH5TZh9WIpwXwscGlo+DPztmYOSbAe2t8U/TfKNJahlvs4DvjnuImZhXfNjXfO3ImrLp97UtSLqmsVKqeuvnWjF2M6WqapdwK5x7X82SQ5U1eS465jJuubHuuZvpdZmXaduKT5QPQKsH1pe1/okSctkKcL9D4GNSS5McjpwPbB3CfYjSTqBRZ+WqarXk/wz4H8ApwGfraonF3s/S2RFTRMNsa75sa75W6m1WdcpSlWNuwZJ0iLzClVJ6pDhLkkdMtyBJOuTPJDkqSRPJvnouGsaluS0JF9N8sVx13JcktVJ7k7y9SQHk7x33DUBJPml9n/4RJI7krxtTHV8NsmxJE8M9Z2TZF+Sp9vz2Sukrv/Q/h8fS/KFJKtXQl1D6z6RpJKct1LqSvKR9m/2ZJJ/v9x1jcJwH3gd+ERVXQRsAm5aYV+Z8FHg4LiLmOHTwJer6l3AxayA+pKsBX4RmKyqdzP4QP/6MZXzOeDKGX07gP1VtRHY35aX2+d4c137gHdX1d8A/jfwyeUuitnrIsl64GeB55e7oOZzzKgryfsYXHV/cVX9FPBrY6hrToY7UFVHq+qR1v4TBkG1drxVDSRZB1wDfGbctRyX5Czgp4HdAFX1F1X1ynir+p5VwNuTrALOAP7POIqoqt8DvjWjewuwp7X3ANcua1HMXldVfaWqXm+LDzK4NmXsdTU3A78MjOXMjxPU9U+BnVX1523MsWUvbASG+wxJNgCXAg+Nt5Lv+Q0Gv9x/Oe5ChlwITAO/1aaLPpPkzHEXVVVHGBxFPQ8cBV6tqq+Mt6o3WFNVR1v7BWDNOIs5gX8M/PdxFwGQZAtwpKq+Nu5aZvhJ4O8leSjJ/0zyt8Zd0GwM9yFJfgT4HeBjVfXtFVDPB4BjVfXwuGuZYRVwGXBrVV0KfIfxTDG8QZvD3sLgzefHgTOT/MPxVjW7GpyDvKLOQ07yLxhMUd6+Amo5A/hV4F+Ou5ZZrALOYTCF+8+Bu5JkvCW9meHeJHkLg2C/varuGXc9zeXAB5M8C9wJXJHkt8dbEjD4MrjDVXX8r5u7GYT9uP0M8MdVNV1V3wXuAf7OmGsa9mKS8wHa84r5cz7JPwI+APx8rYyLX/46gzfpr7Xf/3XAI0n+6lirGjgM3FMDf8Dgr+pl/7B3LoY70N51dwMHq+rXx13PcVX1yapaV1UbGHwweH9Vjf1ItKpeAA4leWfr2szK+Ern54FNSc5o/6ebWQEf9A7ZC2xt7a3AvWOs5XuSXMlg6u+DVfXauOsBqKrHq+rHqmpD+/0/DFzWfvfG7b8B7wNI8pPA6ayMb4h8A8N94HLgFxgcGT/aHlePu6gV7iPA7UkeAy4B/t2Y66H9JXE38AjwOIPf77FcJp7kDuD3gXcmOZxkG7ATeH+Spxn8lbFzhdT1m8CPAvva7/5/WiF1jd0J6vos8BPt9Mg7ga0r5K+dN/DrBySpQx65S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUof8PZB8kc6B2pFUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "lengths = [len(word) for word in words]\n",
    "\n",
    "plt.hist(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting token frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[': 1,\n",
       "         'The': 1,\n",
       "         'Tragedie': 1,\n",
       "         'of': 1,\n",
       "         'Macbeth': 1,\n",
       "         'by': 1,\n",
       "         'William': 1,\n",
       "         'Shakespeare': 1,\n",
       "         '1603': 1,\n",
       "         ']': 1})"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(word_tokenize(text)[:10])\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 1), ('The', 1), ('Tragedie', 1), ('of', 1), ('Macbeth', 1)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tragedie',\n",
       " 'macbeth',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'actus',\n",
       " 'primus',\n",
       " 'scoena',\n",
       " 'prima',\n",
       " 'thunder',\n",
       " 'lightning']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that isalpha will return false for words like \"U.S.A.\" which may not be desireable\n",
    "alpha_only = [token for token in word_tokenize(text.lower()) if token.isalpha()]\n",
    "no_stops = [token for token in alpha_only if token not in stopwords.words('english')]\n",
    "no_stops[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 19), ('thane', 17), ('macbeth', 15)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(no_stops).most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tragedie',\n",
       " 'macbeth',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'actus',\n",
       " 'primus',\n",
       " 'scoena',\n",
       " 'prima',\n",
       " 'thunder',\n",
       " 'lightning']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')[0:10000]\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(text)\n",
    "tokens = [token.text.lower() for token in doc]\n",
    "\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "no_stops = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "no_stops[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming tries to convert words into their root forms by identifying and removing affies while keeping the root meaning of the word.\n",
    "\n",
    "> Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. ([#](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'saw', 'six', 'bat', 'fli', 'through', 'the', 'sky']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I saw six bats flying through the sky\"\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer\n",
    "\n",
    "> Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. ([#](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'saw', 'six', 'bat', 'flying', 'through', 'the', 'sky']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Need to POS tag tokens first:\n",
    "# https://stackoverflow.com/questions/32957895/wordnetlemmatizer-not-returning-the-right-lemma-unless-pos-is-explicit-python\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "[wordnet_lemmatizer.lemmatize(token) for token in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 22), ('thane', 17), ('macbeth', 15)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(lemmatized).most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-PRON-', 'see', 'six', 'bat', 'fly', 'through', 'the', 'sky']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(text)\n",
    "tokens = [token.lemma_ for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing using spaCy\n",
    "\n",
    "via a [DataCamp exercise](https://campus.datacamp.com/courses/feature-engineering-for-nlp-in-python/text-preprocessing-pos-tagging-and-ner?ex=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw six bats flying through the sky</td>\n",
       "      <td>bat fly sky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text preprocessed\n",
       "0  I saw six bats flying through the sky  bat fly sky"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "# spacy's stop_words contains words like \"six\" and \"see\" for some reason...\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stop_words]\n",
    "    return ' '.join(a_lemmas)\n",
    "\n",
    "df = pd.DataFrame({\"text\": [\"I saw six bats flying through the sky\"]})\n",
    "df[\"preprocessed\"] = df[\"text\"].apply(preprocess)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 1:\n",
      "  there: 5\n",
      "  is: 2\n",
      "  a: 0\n",
      "  cat: 1\n",
      "  near: 3\n",
      "  the: 4\n",
      "  cat: 1\n",
      "\n",
      "Article 2:\n",
      "  there: 5\n",
      "  is: 2\n",
      "  a: 0\n",
      "  dog: 6\n",
      "\n",
      "Corpus [[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1)], [(0, 1), (2, 1), (5, 1), (6, 1)]]\n",
      "\n",
      "First doc sorted by occurrences: [(1, 2), (0, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "\n",
      "Top words in first doc:\n",
      "  cat 2\n",
      "  a 1\n",
      "  is 1\n",
      "\n",
      "Total word counts:\n",
      "dict_items([(0, 2), (1, 2), (2, 2), (3, 1), (4, 1), (5, 2), (6, 1)])\n",
      "\n",
      "Top 5 words across corpus:\n",
      "  a 2\n",
      "  cat 2\n",
      "  is 2\n",
      "  there 2\n",
      "  near 1\n"
     ]
    }
   ],
   "source": [
    "raw = [\"There is a cat near the cat\", \"There is a dog\"]\n",
    "articles = [word_tokenize(article.lower()) for article in raw]\n",
    "\n",
    "dictionary = Dictionary(articles)\n",
    "for index, article in enumerate(articles):\n",
    "    print(\"\\nArticle\", str(index + 1) + \":\")\n",
    "    for token in article:\n",
    "        print(\" \", token + \":\", dictionary.token2id.get(token))\n",
    "    \n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# First element of each tuple is the id of the token\n",
    "# Second element is number of times it appears in the article\n",
    "print(\"\\nCorpus\", corpus)\n",
    "\n",
    "bow_doc = sorted(corpus[0], key=lambda w: w[1], reverse=True)\n",
    "print(\"\\nFirst doc sorted by occurrences:\", bow_doc)\n",
    "\n",
    "print(\"\\nTop words in first doc:\")\n",
    "for word_id, word_count in bow_doc[:3]:\n",
    "    print(\" \", dictionary.get(word_id), word_count)\n",
    "    \n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "print(\"\\nTotal word counts:\")\n",
    "print(total_word_count.items())\n",
    "\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "print(\"\\nTop 5 words across corpus:\")\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(\" \", dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article: There is a cat near the cat\n",
      "\n",
      "doc: [(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "\n",
      "tfidf_weights: [(1, 0.8164965809277261), (3, 0.4082482904638631), (4, 0.4082482904638631)]\n",
      "\n",
      "Top 2 terms:\n",
      "cat 0.8164965809277261\n",
      "near 0.4082482904638631\n"
     ]
    }
   ],
   "source": [
    "print(\"article:\", raw[0])\n",
    "\n",
    "doc = corpus[0]\n",
    "print(\"\\ndoc:\", doc)\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf_weights = tfidf[doc]\n",
    "print(\"\\ntfidf_weights:\", tfidf_weights)\n",
    "\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 2 weighted words\n",
    "print(\"\\nTop 2 terms:\")\n",
    "for term_id, weight in sorted_tfidf_weights[:2]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying parts of speech\n",
    "\n",
    "* See [list of tags](https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/) for what each acronym represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('John', 'NNP'),\n",
       "  ('Gleen', 'NNP'),\n",
       "  ('as', 'IN'),\n",
       "  ('an', 'DT'),\n",
       "  ('astronaut', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('NASA', 'NNP'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"John Gleen as an astronaut in NASA.\"]\n",
    "parts_of_speech = [nltk.pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "parts_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['John', 'PROPN'],\n",
       " ['Gleen', 'PROPN'],\n",
       " ['as', 'SCONJ'],\n",
       " ['an', 'DET'],\n",
       " ['astronaut', 'NOUN'],\n",
       " ['in', 'ADP'],\n",
       " ['NASA', 'PROPN'],\n",
       " ['.', 'PUNCT']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(sentences[0])\n",
    "tokens = [[token.text, token.pos_] for token in doc]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Named Entities with nltk\n",
    "\n",
    "Name Entities are people, organizations, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE John/NNP Gleen/NNP)\n",
      "(NE NASA/NNP)\n"
     ]
    }
   ],
   "source": [
    "chunked_sentences = nltk.ne_chunk_sents(parts_of_speech, binary=True)\n",
    "\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Named Entities with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON John Gleen\n",
      "ORG NASA\n"
     ]
    }
   ],
   "source": [
    "# Need to run `$ python3 -m spacy download en` beforehand\n",
    "nlp = spacy.load(\"en\", tagger=False, parser=False, matcher=False)\n",
    "\n",
    "doc = nlp(\"John Gleen as an astronaut in NASA.\")\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding text as a bag of words with Count Vectorizer\n",
    "\n",
    "CountVectorizer will count how many times each word is in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['chews', 'dogs', 'runs', 'scratches', 'sola', 'toy']\n",
      "\n",
      "Vocabulary: {'sola': 4, 'runs': 2, 'scratches': 3, 'chews': 0, 'toy': 5, 'dogs': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chews</th>\n",
       "      <th>dogs</th>\n",
       "      <th>runs</th>\n",
       "      <th>scratches</th>\n",
       "      <th>sola</th>\n",
       "      <th>toy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chews  dogs  runs  scratches  sola  toy\n",
       "0      0     0     1          0     1    0\n",
       "1      0     0     0          1     2    0\n",
       "2      1     1     0          0     1    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stop_words=\"english\" will remove stop words like \"is\" and \"a\" here\n",
    "# It defaults to lowercasing all words. Lots of other params available too.\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "X = [\"Sola runs\", \"Sola scratches Sola\", \"Sola chews toy dogs\"]\n",
    "X_transformed = count_vectorizer.fit_transform(X)\n",
    "\n",
    "print(\"Feature Names:\", count_vectorizer.get_feature_names())\n",
    "print(\"\\nVocabulary:\", count_vectorizer.vocabulary_)\n",
    "\n",
    "df = pd.DataFrame(X_transformed.todense(), columns=count_vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying 1 and 2 grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['chews', 'chews toy', 'dogs', 'runs', 'scratches', 'scratches sola', 'sola', 'sola chews', 'sola runs', 'sola scratches', 'toy', 'toy dogs']\n",
      "\n",
      "Vocabulary: {'sola': 6, 'runs': 3, 'sola runs': 8, 'scratches': 4, 'sola scratches': 9, 'scratches sola': 5, 'chews': 0, 'toy': 10, 'dogs': 2, 'sola chews': 7, 'chews toy': 1, 'toy dogs': 11}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chews</th>\n",
       "      <th>chews toy</th>\n",
       "      <th>dogs</th>\n",
       "      <th>runs</th>\n",
       "      <th>scratches</th>\n",
       "      <th>scratches sola</th>\n",
       "      <th>sola</th>\n",
       "      <th>sola chews</th>\n",
       "      <th>sola runs</th>\n",
       "      <th>sola scratches</th>\n",
       "      <th>toy</th>\n",
       "      <th>toy dogs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chews  chews toy  dogs  runs  scratches  scratches sola  sola  sola chews  \\\n",
       "0      0          0     0     1          0               0     1           0   \n",
       "1      0          0     0     0          1               1     2           0   \n",
       "2      1          1     1     0          0               0     1           1   \n",
       "\n",
       "   sola runs  sola scratches  toy  toy dogs  \n",
       "0          1               0    0         0  \n",
       "1          0               1    0         0  \n",
       "2          0               0    1         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "\n",
    "X = [\"Sola runs\", \"Sola scratches Sola\", \"Sola chews toy dogs\"]\n",
    "X_transformed = count_vectorizer.fit_transform(X)\n",
    "\n",
    "print(\"Feature Names:\", count_vectorizer.get_feature_names())\n",
    "print(\"\\nVocabulary:\", count_vectorizer.vocabulary_)\n",
    "\n",
    "df = pd.DataFrame(X_transformed.todense(), columns=count_vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "\n",
    "X = imdb[\"review\"]\n",
    "y = imdb[\"sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 features: ['00', '000', '00000000000', '0000000000001', '00001', '00015', '000dm', '001', '003830', '0069']\n",
      "Train shape: (35000, 88028)\n",
      "Train shape: (15000, 88028)\n",
      "Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"First 10 features:\", count_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "print(\"Train shape:\", X_train_transformed.shape)\n",
    "print(\"Train shape:\", X_test_transformed.shape)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "print(\"Score: {:.2f}\".format(nb.score(X_test_transformed, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TfidfVectorizer\n",
    "\n",
    "See TF-IDF notebook for more in depth examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 features: ['00', '000', '00000000000', '0000000000001', '00001', '00015', '000dm', '001', '003830', '0069']\n",
      "Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "# max_df: \"When building the vocabulary ignore terms that have a document frequency strictly higher than \n",
    "# the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents \n",
    "# a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\"\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "X_train_transformed = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"First 10 features:\", tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "print(\"Score: {:.2f}\".format(nb.score(X_test_transformed, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "0-1 score where 1 = exact same words in document, 0 = none of the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>chases</th>\n",
       "      <th>dog</th>\n",
       "      <th>frog</th>\n",
       "      <th>is</th>\n",
       "      <th>there</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.517856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.47363</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat    chases       dog      frog        is     there\n",
       "0  0.00000  0.000000  0.680919  0.000000  0.517856  0.517856\n",
       "1  0.57735  0.000000  0.000000  0.000000  0.577350  0.577350\n",
       "2  0.47363  0.622766  0.000000  0.622766  0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix:\n",
      "\n",
      " [[1.         0.59796874 0.        ]\n",
      " [0.59796874 1.         0.27345018]\n",
      " [0.         0.27345018 1.        ]]\n",
      "\n",
      "Linear kernel:\n",
      "\n",
      " [[1.         0.59796874 0.        ]\n",
      " [0.59796874 1.         0.27345018]\n",
      " [0.         0.27345018 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"There is a dog\", \"There is a cat\", \"A cat chases a frog\"]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "display(df)\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(\"Cosine similarity matrix:\\n\\n\", cosine_sim)\n",
    "\n",
    "# Notice how both linear_kernel and cosine_similarity produced the same result. \n",
    "# However, linear_kernel took a smaller amount of time to execute. When you're \n",
    "# working with a very large amount of data and your vectors are in the tf-idf \n",
    "# representation, it is good practice to default to linear_kernel to improve performance.\n",
    "# via https://campus.datacamp.com/courses/feature-engineering-for-nlp-in-python/tf-idf-and-similarity-scores?ex=9\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print(\"\\nLinear kernel:\\n\\n\", cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a list of similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is a dog', 'A cat chases a frog']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = pd.Series(corpus)\n",
    "indices = pd.Series(series.index, index=series)\n",
    "scores = cosine_sim[indices[\"There is a cat\"]]\n",
    "\n",
    "n = 2\n",
    "\n",
    "# Figure out the indices that would sort smallest to largest, then reverse that list,\n",
    "# and grab top top n items excluding the first item (which will always be 1 because it's\n",
    "# the cosine similarity to itself)\n",
    "top_similar_scores_indices = list(scores.argsort()[::-1][1:n+1])\n",
    "list(series[top_similar_scores_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-111-fcd63f262692>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  doc1.similarity(doc2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8750411709609652"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This has to be downloaded separately:\n",
    "# python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "doc1 = nlp(\"The cat ran\")\n",
    "doc2 = nlp(\"The dog ran\")\n",
    "\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_transformed, y_train)\n",
    "nb.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flesch_score': 78.55659919028342,\n",
       " 'fleschkincaid_score': 5.531821862348178,\n",
       " 'gunningfog_score': 7.7910931174089075,\n",
       " 'smog_score': 8.371475516178862,\n",
       " 'dalechall_score': 7.861219028340081}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you get `division by zero` error it means the text isn't long enough\n",
    "text = \"\\nThe gods had condemned Sisyphus to ceaselessly rolling a rock to the top of a mountain, whence the stone would fall back of its own weight. They had thought with some reason that there is no more dreadful punishment than futile and hopeless labor. If one believes Homer, Sisyphus was the wisest and most prudent of mortals. According to another tradition, however, he was disposed to practice the profession of highwayman. I see no contradiction in this. Opinions differ as to the reasons why he became the futile laborer of the underworld. To begin with, he is accused of a certain levity in regard to the gods. He stole their secrets. Egina, the daughter of Esopus, was carried off by Jupiter. The father was shocked by that disappearance and complained to Sisyphus. He, who knew of the abduction, offered to tell about it on condition that Esopus would give water to the citadel of Corinth. To the celestial thunderbolts he preferred the benediction of water. He was punished for this in the underworld. Homer tells us also that Sisyphus had put Death in chains. Pluto could not endure the sight of his deserted, silent empire. He dispatched the god of war, who liberated Death from the hands of her conqueror. It is said that Sisyphus, being near to death, rashly wanted to test his wife\\'s love. He ordered her to cast his unburied body into the middle of the public square. Sisyphus woke up in the underworld. \"\n",
    "\n",
    "scores = Textatistic(text).scores\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
