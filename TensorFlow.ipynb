{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "* Open source library for graph-based numerical computation (??)\n",
    "* Low and high level APIs\n",
    "  * Addition, multiplication, differentiation\n",
    "  * Machine learning models\n",
    "\n",
    "References:\n",
    "\n",
    "* [TensorFlow Website](https://www.tensorflow.org/)\n",
    "* [Introduction to TensorFlow in Python](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python) // many of the examples in here are from this course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining tensors\n",
    "\n",
    "A tensor is a genrealization of vectors and matrices. Like a collection of numbers arranged in a particular shape. Imagine a loaf of bread, cut into slices, and the slices are cut into 9 pieces. A piece of the slice is a 0 dimensional vector. A row or column of pieces is a 1 dimensional vector. A whole slice is a 2 dimensional vector. The whole loaf is a 3 dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0d\n",
    "\n",
    "Here is a \"scalar\" or \"rank-0\" tensor . A scalar contains a single value, and no \"axes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant(5).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d\n",
    "\n",
    "A \"vector\" or \"rank-1\" tensor is like a list of values. A vector has 1-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant([2.0, 3.0, 4.0]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant([5]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d\n",
    "\n",
    "A \"matrix\" or \"rank-2\" tensor has 2-axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float16).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "d3 = tf.ones((2, 2, 2))\n",
    "print(d3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "* A constant is the simplest category of tensor\n",
    "  * It's not trainable (??)\n",
    "  * Can have any dimension\n",
    "* `shape` is `[rows, columns]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 3]\n",
      " [3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant(3, shape=[2, 3]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant([1, 2, 3, 4], shape=[2, 2]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.zeros([2, 2]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(4, shape=[2, 3]).numpy()\n",
    "print(tf.zeros_like(a).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Lists work:\n",
    "print(tf.ones([2, 2]).numpy())\n",
    "\n",
    "# So do tuples:\n",
    "print()\n",
    "print(tf.ones((2, 2)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones((2, 2)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ones Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(4, shape=[2, 3]).numpy()\n",
    "print(tf.ones_like(a).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 7]\n",
      " [7 7 7]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.fill([3, 3], 7).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a constnat from a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int64'>\n",
      "[5 6 7]\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([5, 6, 7])\n",
    "b = tf.constant(arr)\n",
    "print(b.dtype)\n",
    "print(b.numpy())\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "* Unlike a constant, a variable's value can be changed\n",
    "* Data type and shape are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varible: [1 2 3]\n",
      "Constant: 2\n",
      "Multiplication method 1: [2 4 6]\n",
      "Multiplication method 2: [2 4 6]\n",
      "Multiplication method 3: [2 4 6]\n",
      "Multiplication method 4: [2 4 6]\n"
     ]
    }
   ],
   "source": [
    "# dtype could also be like tf.float32\n",
    "a0 = tf.Variable([1, 2, 3], dtype=tf.int16)\n",
    "print(\"Varible:\", a0.numpy())\n",
    "\n",
    "b = tf.constant(2, tf.int16)\n",
    "print(\"Constant:\", b.numpy())\n",
    "\n",
    "c1 = a0 * b\n",
    "print(\"Multiplication method 1:\", c1.numpy())\n",
    "print(\"Multiplication method 2:\", tf.multiply(a0, b).numpy())\n",
    "\n",
    "# \"Note that tensorflow 2 allows you to use data as either a numpy array \n",
    "# or a tensorflow constant object. Using a constant will ensure that any \n",
    "# operations performed with that object are done in tensorflow.\"\n",
    "print(\"Multiplication method 3:\", tf.multiply([1, 2, 3], [2]).numpy())\n",
    "print(\"Multiplication method 4:\", tf.multiply(np.array([1, 2, 3]), np.array([2])).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1])\n",
    "b = tf.constant([2])\n",
    "print(tf.add(a, b).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 15]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 5])\n",
    "b = tf.constant([3, 10])\n",
    "print(tf.add(a, b).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 50]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 5])\n",
    "b = tf.constant([3, 10])\n",
    "print(tf.multiply(a, b).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "\n",
      "b:\n",
      " [[5 6]\n",
      " [7 8]]\n",
      "\n",
      "a + b:\n",
      " [[ 6  8]\n",
      " [10 12]]\n",
      "\n",
      "a + b:\n",
      " [[ 6  8]\n",
      " [10 12]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 2, 3, 4], shape=[2, 2])\n",
    "b = tf.constant([5, 6, 7, 8], shape=[2, 2])\n",
    "\n",
    "print(\"a:\\n\", a.numpy())\n",
    "print(\"\\nb:\\n\", b.numpy())\n",
    "print(\"\\na + b:\\n\", tf.add(a, b).numpy())\n",
    "print(\"\\na + b:\\n\", (a + b).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "* [Multiplying a Matrix by Another Matrix](https://www.mathsisfun.com/algebra/matrix-multiplying.html)\n",
    "* To multiple matrix `a` by matrix `b`, `b` must have the same number of rows as `a` has columns since we're taking a dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "\n",
      "b:\n",
      " [[5 6]\n",
      " [7 8]]\n",
      "\n",
      "a * b:\n",
      " [[19 22]\n",
      " [43 50]]\n",
      "\n",
      "a * b:\n",
      " [[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 2, 3, 4], shape=[2, 2])\n",
    "b = tf.constant([5, 6, 7, 8], shape=[2, 2])\n",
    "\n",
    "print(\"a:\\n\", a.numpy())\n",
    "print(\"\\nb:\\n\", b.numpy())\n",
    "print(\"\\na * b:\\n\", tf.matmul(a, b).numpy())\n",
    "print(\"\\na * b:\\n\", (a @ b).numpy())\n",
    "\n",
    "# 1 * 5 + 2 * 7 = 19\n",
    "# 1 * 6 + 2 * 8 = 22\n",
    "# 3 * 5 + 4 * 7 = 34\n",
    "# 3 * 6 + 4 * 8 = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      " [[2 3]\n",
      " [4 5]]\n",
      "\n",
      "coefficients:\n",
      " [[1000]\n",
      " [  50]]\n",
      "\n",
      "predictions:\n",
      " [[2150]\n",
      " [4250]]\n"
     ]
    }
   ],
   "source": [
    "features = tf.constant([2, 3, 4, 5], shape=[2, 2])\n",
    "coefficients = tf.constant([1000, 50], shape=[2, 1])\n",
    "predictions = tf.matmul(features, coefficients)\n",
    "\n",
    "print(\"features:\\n\", features.numpy())\n",
    "print(\"\\ncoefficients:\\n\", coefficients.numpy())\n",
    "print(\"\\npredictions:\\n\", predictions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing over tensor dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.ones([2, 3, 4])\n",
    "\n",
    "# 24 because it's a 2 x 3 x 4 dimensional tensor with each element being 1\n",
    "tf.reduce_sum(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimension 0:\n",
      " [[2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]]\n",
      "\n",
      "Dimension 1:\n",
      " [[3. 3. 3. 3.]\n",
      " [3. 3. 3. 3.]]\n",
      "\n",
      "Dimension 2:\n",
      " [[4. 4. 4.]\n",
      " [4. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# We reduce the size of the tensor by summing over one of its dimensions\n",
    "print(\"\\nDimension 0:\\n\", tf.reduce_sum(a, 0).numpy()) # A 3x4 tensor\n",
    "print(\"\\nDimension 1:\\n\", tf.reduce_sum(a, 1).numpy()) # A 2x4 tensor\n",
    "print(\"\\nDimension 2:\\n\", tf.reduce_sum(a, 2).numpy()) # A 2x3 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " [[3 4]\n",
      " [5 6]]\n",
      "\n",
      "Total sum:\n",
      " 18\n",
      "\n",
      "Column sums:\n",
      " [ 8 10]\n",
      "\n",
      "Row sums:\n",
      " [ 7 11]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([3, 4, 5, 6], shape=[2, 2])\n",
    "print(\"a:\\n\", a.numpy())\n",
    "\n",
    "print(\"\\nTotal sum:\\n\", tf.reduce_sum(a).numpy())\n",
    "\n",
    "# Reducing along rows (axis=0) means we're squeezing from bottom and top so that\n",
    "# the separate rows become one row. This gives is the column sums.\n",
    "print(\"\\nColumn sums:\\n\", tf.reduce_sum(a, 0).numpy())\n",
    "\n",
    "# Reducing along columns (axis=1) means we're squeezing from left and right so that\n",
    "# the separate columns become one column. This gives us the row sums.\n",
    "print(\"\\nRow sums:\\n\", tf.reduce_sum(a, 1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "Computes the slope of a function at a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope at -1.0: -2.0:\n"
     ]
    }
   ],
   "source": [
    "# We'll find the minumum for y = x ** 2\n",
    "\n",
    "# Define x\n",
    "x = tf.Variable(-1.0)\n",
    "\n",
    "# Define y within an instance of GradientTape\n",
    "# This allows us to computer the rate of change of y with respect to x\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.multiply(x, x)\n",
    "\n",
    "# Evaluate the gradient of y with respect to x at x = -1\n",
    "g = tape.gradient(y, x)\n",
    "print(\"Slope at {}: {}:\".format(x.numpy(), g.numpy()))\n",
    "\n",
    "# Recall from calc that the derivative for y = x ** 2 is 2x\n",
    "# So the derivative at x = -1 is 2 * -1 = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape\n",
    "\n",
    "Reshapes a tensor (like a 10x10 into a 100x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2x2:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Reshaped into 4x1 using list:\n",
      " [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "\n",
      "Rehsaped into a 4x1 using tuple:\n",
      " [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 2, 3, 4], shape=[2, 2])\n",
    "print(\"Original 2x2:\\n\", a.numpy())\n",
    "print(\"\\nReshaped into 4x1 using list:\\n\", tf.reshape(a, [4, 1]).numpy())\n",
    "print(\"\\nRehsaped into a 4x1 using tuple:\\n\", tf.reshape(a, (4, 1)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random\n",
    "\n",
    "Populates a tensor with entries drawn from a probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8147571  0.4983344 ]\n",
      " [0.622983   0.22424698]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform([2, 2])\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float between 0 and n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.0342765 3.837868 ]\n",
      " [5.8309007 3.4020329]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform([2, 2], maxval=10)\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer between 0 and n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 5]\n",
      " [7 1]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform([2, 2], maxval=10, dtype=\"int32\")\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer to boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True  True]\n"
     ]
    }
   ],
   "source": [
    "series = pd.Series([1, 0, 1, 1])\n",
    "print(tf.cast(series, tf.bool).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23. 45. 67.]\n"
     ]
    }
   ],
   "source": [
    "series = pd.Series([23, 45, 67])\n",
    "print(tf.cast(series, tf.float32).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that if you pass lists of integers then TensorFlow will return an integer (12) not 12.5\n",
    "targets = [10.0, 12.0]\n",
    "predictions = [14.0, 9.0]\n",
    "tf.keras.losses.mse(targets, predictions).numpy()\n",
    "\n",
    "# Sum the squares of predicitons minus targets: (14 - 10)** 2 + (9 - 12) ** 2 = 16 + 9 = 25\n",
    "# Divide by number of observations = 25 / 2 = 12.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [DataCamp example](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63343?ex=6):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "features = tf.constant([1, 2, 3, 4, 5], tf.float32)\n",
    "targets = tf.constant([2, 4, 6, 8, 10], tf.float32)\n",
    "scalar = tf.Variable(1.0, tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def model(scalar, features = features):\n",
    "    return scalar * features\n",
    "\n",
    "# Define a loss function\n",
    "def loss_function(scalar, features = features, targets = targets):\n",
    "    # Compute the predicted values\n",
    "    predictions = model(scalar, features)\n",
    "    \n",
    "    # Return the mean absolute error loss\n",
    "    return tf.keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Evaluate the loss function and print the loss\n",
    "print(loss_function(scalar).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified example inspired by a [DataCamp exercise](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63343?ex=9):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "MSE after 0 iterations: 1052.125\n",
      "MSE after 50 iterations: 3.633875\n",
      "MSE after 100 iterations: 0.40788904\n",
      "MSE after 150 iterations: 0.03056418\n",
      "MSE after 200 iterations: 0.0010345107\n",
      "MSE after 250 iterations: 1.1735907e-05\n",
      "MSE after 300 iterations: 1.2565943e-08\n",
      "\n",
      "Estimated intercept: 9.999792\n",
      "Estimated slope:  5.0000343\n"
     ]
    }
   ],
   "source": [
    "x = np.array(range(10))\n",
    "y = 10 + x * 5\n",
    "\n",
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope, features = x):\n",
    "    return intercept + slope * features\n",
    "\n",
    "# Set loss_function() to take the variables as arguments\n",
    "def loss_function(intercept, slope, features, targets):\n",
    "    # Set the predicted values\n",
    "    predictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    "    # Return the mean squared error loss\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "# Initialize an Adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(0.5)\n",
    "\n",
    "# The optimizer will adjust the intercept and slope\n",
    "intercept = tf.Variable(0, dtype=tf.float32)\n",
    "slope = tf.Variable(0, dtype=tf.float32)\n",
    "\n",
    "for i in range(301):\n",
    "    # Apply minimize, pass the loss function, and supply the variables\n",
    "    opt.minimize(lambda: loss_function(intercept, slope, x, y), var_list=[intercept, slope])\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        current_loss = loss_function(intercept, slope, x, y).numpy()\n",
    "        print(\"MSE after\", i, \"iterations:\", current_loss)\n",
    "\n",
    "\n",
    "print(\"\\nEstimated intercept:\", intercept.numpy())\n",
    "print(\"Estimated slope: \", slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: [0 1 2 3 4 5 6 7 8 9]\n",
      "x2: [30 31 32 33 34 35 36 37 38 39]\n",
      "actual params: [10, 5, 25]\n",
      "y: [ 760  790  820  850  880  910  940  970 1000 1030]\n",
      "\n",
      "Training...\n",
      "MSE after 0 iterations: 772563.25\n",
      "MSE after 250 iterations: 762.1769\n",
      "MSE after 500 iterations: 130.75668\n",
      "MSE after 750 iterations: 11.037022\n",
      "MSE after 1000 iterations: 0.4575019\n",
      "MSE after 1250 iterations: 0.0088784555\n",
      "MSE after 1500 iterations: 7.480383e-05\n",
      "MSE after 1750 iterations: 3.0845405e-07\n",
      "MSE after 2000 iterations: 3.837049e-08\n",
      "\n",
      "Estimated params: [27.377342   5.5793157 24.420744 ]\n",
      "\n",
      "Predictions: [ 759.99963  789.99976  819.99976  849.9998   879.9999   909.99994\n",
      "  940.       970.00006 1000.0001  1030.0002 ]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.arange(0, 10)\n",
    "x2 = np.arange(30, 40)\n",
    "actual_params = [10, 5, 25]\n",
    "y = actual_params[0] + actual_params[1] * x1 + actual_params[2] * x2\n",
    "\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"actual params:\", actual_params)\n",
    "print(\"y:\", y)\n",
    "\n",
    "# Define a linear regression model\n",
    "def linear_regression(params, feature1, feature2):\n",
    "    return params[0] + params[1] * feature1 + params[2] * feature2\n",
    "\n",
    "# Set loss_function() to take the variables as arguments\n",
    "def loss_function(params, feature1, feature2, targets):\n",
    "    # Set the predicted values\n",
    "    predictions = linear_regression(params, feature1, feature2)\n",
    "    \n",
    "    # Return the mean squared error loss\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "# Initialize an Adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(0.5)\n",
    "\n",
    "# The optimizer will adjust the intercept and slope\n",
    "params = tf.Variable([0, 0, 0], dtype=tf.float32)\n",
    "\n",
    "for i in range(2001):\n",
    "    # Apply minimize, pass the loss function, and supply the variables\n",
    "    opt.minimize(lambda: loss_function(params, x1, x2, y), var_list=[params])\n",
    "\n",
    "    if i % 250 == 0:\n",
    "        current_loss = loss_function(params, x1, x2, y).numpy()\n",
    "        print(\"MSE after\", i, \"iterations:\", current_loss)\n",
    "\n",
    "# It comes up with different params that still work\n",
    "print(\"\\nEstimated params:\", params.numpy())\n",
    "print(\"\\nPredictions:\", linear_regression(params, x1, x2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a neural network using low level functions\n",
    "\n",
    "Imagine 3 inputs, 2 hidden layer neurons, and 1 output layer\n",
    "\n",
    "Inspired by [Introduction to TensorFlow example exercise](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63344?ex=2).\n",
    "\n",
    "First, we feed the inputs to the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      " [[ 2. 10. 43.]]\n",
      "\n",
      "hidden_layer_bias: 1.0\n",
      "\n",
      "hidden_layer_weights:\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "hidden_layer_inputs_times_weights:\n",
      " [[55. 55.]]\n",
      "\n",
      "dense1:\n",
      " [[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[2, 10, 43]], np.float32)\n",
    "print(\"inputs:\\n\", inputs)\n",
    "\n",
    "# We figure out the total net input to each hidden layer neuron, squash the \n",
    "# total net input using an activation function (here we use the sigmoid function), \n",
    "# then repeat the process with the output layer neurons.\n",
    "\n",
    "# Initialize bias for the hidden layer\n",
    "hidden_layer_bias = tf.Variable(1.0)\n",
    "print(\"\\nhidden_layer_bias:\", hidden_layer_bias.numpy())\n",
    "\n",
    "# These are the weights from the inputs to the hidden layer neurons\n",
    "# It's 3 rows by 2 columns because there are 3 inputs feeding into 2 neurons\n",
    "hidden_layer_weights = tf.Variable(tf.ones((3, 2)))\n",
    "print(\"\\nhidden_layer_weights:\\n\", hidden_layer_weights.numpy())\n",
    "\n",
    "# Perform matrix multiplication of the features and weights1\n",
    "hidden_layer_inputs_times_weights = tf.matmul(inputs, hidden_layer_weights)\n",
    "# 1 * 2 + 1 * 10 + 1 * 43 = 55\n",
    "print(\"\\nhidden_layer_inputs_times_weights:\\n\", hidden_layer_inputs_times_weights.numpy())\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "# Because the value is greater than 1, it gets squashed to 1\n",
    "hidden_layer_outputs = tf.keras.activations.sigmoid(hidden_layer_inputs_times_weights + hidden_layer_bias)\n",
    "print(\"\\ndense1:\\n\", hidden_layer_outputs.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we feed the outputs of the hidden layer to the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 0.953\n",
      "actual: 1\n"
     ]
    }
   ],
   "source": [
    "output_layer_bias = tf.Variable(1.0)\n",
    "output_layer_weights = tf.Variable(tf.ones((2, 1)))\n",
    "output_layer_inputs_times_weights = tf.matmul(hidden_layer_outputs, output_layer_weights)\n",
    "prediction = tf.keras.activations.sigmoid(output_layer_inputs_times_weights + output_layer_bias)\n",
    "\n",
    "# \"Our model produces predicted values in the interval between 0 and 1. For the example we considered, \n",
    "# the actual value was 1 and the predicted value was a probability between 0 and 1. This, of course, \n",
    "# is not meaningful, since we have not yet trained our model's parameters.\"\n",
    "print('prediction: {:.3f}'.format(prediction.numpy()[0,0]))\n",
    "print('actual: 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the high level Keras API\n",
    "\n",
    "Here we feed the inputs into the same 2-neuron hidden layer, then the 1-neuron output layer.\n",
    "We could easily add other hidden layers as well, feeding in the output of prior hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 0.520\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[2, 10, 43]], np.float32)\n",
    "# Hidden layers typically use Rectified Linear Unit (ReLu) Activation activation function which is max(0, value)\n",
    "hidden_layer = tf.keras.layers.Dense(2, activation=\"sigmoid\")(inputs)\n",
    "# If this were a classification problem with 2+ output classes, we would use softmax here\n",
    "# If we did, outputs could be interpreted as class probabilities in multiclass classification problems\n",
    "# (each rows values would output to 1 aka 100%)\n",
    "prediction = tf.keras.layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "# The output changes each time because we're using an untrained model with randomly initialized parameters\n",
    "print('prediction: {:.3f}'.format(prediction.numpy()[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More low level training\n",
    "\n",
    "Also a via [DataCamp exercise](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63344?ex=14):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features = borrower_features):\n",
    "    # Apply relu activation functions to layer 1\n",
    "    layer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
    "    # Apply dropout\n",
    "    dropout = keras.layers.Dropout(0.25)(layer1)\n",
    "    return keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
    "    predictions = model(w1, b1, w2, b2)\n",
    "    # Pass targets and predictions to the cross entropy loss\n",
    "    return keras.losses.binary_crossentropy(targets, predictions)\n",
    "\n",
    "Train the model\n",
    "for j in range(100):\n",
    "    # Complete the optimizer\n",
    "    opt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions with model\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets, model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout function\n",
    "\n",
    "Helps prevent overfitting by randomly drops weights connected to certain nodes in a layer during the training process. Force network to develop more robust rules for classification. Tends to improve out of sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "# ... other layers\n",
    "\n",
    "# Drop weights connected to 25% of the nodes randomly\n",
    "dropout1 = tf.keras.layers.Dropout(0.25)(dense2)\n",
    "\n",
    "# ... then pass that to the output layer\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "* Stochastic Gradient Descent (SGD)\n",
    "  * Improved version of gradient descent that is less likely to get stuck in local minimums. For simple problems, SGD algorithms work well.\n",
    "  * `keras.optimizers.SGD(learning_rate=0.01)`\n",
    "  * `learning_rate` - between 0.5 and 0.001. Higher introduces more force.\n",
    "  * Simpler and easy to interpret vs modern optimizations\n",
    "* Root Mean Squared (RMS) optimizer\n",
    "  * Applies different learning rates to each feature which is useful for high dimensional problems\n",
    "  * `keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)`\n",
    "  * `learning_rate`\n",
    "  * `momentum`\n",
    "  * `decay` - setting a low value prevents momentum from accumulating over long periods of the training process\n",
    "* Adaptive Moment (Adam) optimizer\n",
    "  * Generally a good first choice\n",
    "  * Require 10x many iterations to achieve similar loss vs SGD\n",
    "  * `beta1` - lowers momentum\n",
    "  * Performs well with default parameter values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw numbers from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43805295,  1.052406  , -0.26469442],\n",
       "       [ 1.0663421 ,  1.1168493 ,  0.3420491 ],\n",
       "       [-1.3589494 , -0.19850916, -0.09751613]], dtype=float32)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal([3, 3]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw numbers from a normal distribution\n",
    "\n",
    ".. And ignore very large and very small values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47778726,  0.17757209,  1.0410289 ],\n",
       "       [-0.02693135,  0.16252817, -0.59477305],\n",
       "       [-1.1060272 , -0.6403249 ,  0.8212007 ]], dtype=float32)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.truncated_normal([3, 3]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential API\n",
    "\n",
    "via [DataCamp](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63345?ex=2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "# Define a Keras sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "# The input shape represents the number of inputs which in this example\n",
    "# is 784 because we're analyzing a 28x28 image that's reshaped\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation=\"sigmoid\", input_shape=(784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "# We could also set the `metrics` argument to ['accuracy'] since\n",
    "# reporting on the loss isn't that useful.\n",
    "# Optimizer could also be `SGD` or `RMSprop`\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Alternative syntax for passing arguments for the optimizer\n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "via [DataCamp](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63345?ex=5):\n",
    "\n",
    "Only two required arguments for `fit`:\n",
    "\n",
    "1. `features`\n",
    "2. `labels`\n",
    "\n",
    "But many optional including:\n",
    "\n",
    "3. `batch_size` - the number of examples in each batch (32 by default). For example, imagine your training data consists of 128 samples. With the default batch size of 32, there would be 4 batches.\n",
    "4. `epochs` - the number of times you train on the full set of batches. This allows the model to revisit the same batches but with different model weights and possibly optimizer parameters since they are optimized after each batch.\n",
    "5. `validation_split` - selecting a value of 0.2 will put 20% of the data in the validation set. You can see how well the model performs on both the data it was trained on (the training set) and data it was not trained on (the validation set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "model.fit(image_features, image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on the test set\n",
    "\n",
    "Provides you with further assurance that you have not overfitted.\n",
    "\n",
    "\"Notice that the gap between the test and train set losses is high for large_model, suggesting that overfitting may be an issue. Furthermore, both test and train set performance is better for large_model. This suggests that we may want to use large_model, but reduce the number of training epochs\" [#](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63345?ex=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "\n",
    "\"In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this.\" [#](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63345?ex=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators\n",
    "\n",
    "via [DataCamp](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63345?ex=11):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "    # Define the labels\n",
    "    labels = np.array(housing[\"price\"])\n",
    "    # Define the features\n",
    "    features = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing[\"bathrooms\"])}\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "# Define the model and set the number of steps\n",
    "model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Note that you have other premade estimator options, such as BoostedTreesRegressor(), and can also create your own custom estimators.\"[#](https://campus.datacamp.com/courses/introduction-to-tensorflow-in-python/63345?ex=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "# Define the model and set the number of steps\n",
    "model = estimator.LinearRegressor(feature_columns=feature_list)\n",
    "model.train(input_fn, steps=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
